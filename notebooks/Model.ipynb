{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-04T23:26:49.428774Z",
     "start_time": "2020-06-04T23:26:49.426270Z"
    }
   },
   "source": [
    "# Settings, Directory Specs, and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:56.086693Z",
     "start_time": "2020-06-12T10:20:55.182711Z"
    }
   },
   "outputs": [],
   "source": [
    "# 0 = no streamlit\n",
    "# 1 = test user inputs\n",
    "# 2 = run in streamlit\n",
    "streamlit_status = 0\n",
    "#file_name_pickle_read = 'model_2020_06_06_1105.pickle'\n",
    "do_plots = 1\n",
    "\n",
    "dir_read = '/Users/rachellehorwitz/Documents/ViTalErt/data/filtered05/'\n",
    "#dir_read = '/Users/rachellehorwitz/Documents/VTAlert/over18_eicu/'\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import auc, confusion_matrix, plot_confusion_matrix, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve, train_test_split, ShuffleSplit, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pyprojroot import here\n",
    "\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:56.090560Z",
     "start_time": "2020-06-12T10:20:56.088757Z"
    }
   },
   "outputs": [],
   "source": [
    "## LightGBM - same tool. Microsoft Research built their own version of XGradientBoost. \n",
    "## XGradientBoost\n",
    "## If you look at kaggle challenges, the types of modles that get the highest accuracy\n",
    "# on lab-like conditions where you have a fixed dataset, the types of models that win are gradient\n",
    "# boosting (XGBoost) are NNs. People use XGBoost. Mention that it would be a route to go down\n",
    "# but don't use it because it requires a particular data format\n",
    "# Look at logistic regression equation. Look at coefficients I'm estimating. Use statsmodels\n",
    "#### scikit learn implementation of p-values from the coefficients\n",
    "#### In the end, I want to convince someone that an intervention is necessary\n",
    "#### Value-add of ML is that a doctor could have a simple hypothesis, like \"this is age-dependent\"\n",
    "#### I could throw subset of data in and I wouldn't get stat significance\n",
    "#### Tie it back to statistical significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:56.097571Z",
     "start_time": "2020-06-12T10:20:56.093019Z"
    }
   },
   "outputs": [],
   "source": [
    "def now_to_str():\n",
    "    now = str(datetime.now())\n",
    "    return now[0:4] + '_' + now[5:7] + '_' + now[8:10] + '_' + now[11:13] + now[14:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:56.103663Z",
     "start_time": "2020-06-12T10:20:56.099615Z"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.datasets import load_digits\n",
    "# from sklearn.model_selection import learning_curve\n",
    "# from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "# def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "#                         n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "#     \"\"\"\n",
    "#     Generate 3 plots: the test and training learning curve, the training\n",
    "#     samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "#         An object of that type which is cloned for each validation.\n",
    "\n",
    "#     title : string\n",
    "#         Title for the chart.\n",
    "\n",
    "#     X : array-like, shape (n_samples, n_features)\n",
    "#         Training vector, where n_samples is the number of samples and\n",
    "#         n_features is the number of features.\n",
    "\n",
    "#     y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "#         Target relative to X for classification or regression;\n",
    "#         None for unsupervised learning.\n",
    "\n",
    "#     axes : array of 3 axes, optional (default=None)\n",
    "#         Axes to use for plotting the curves.\n",
    "\n",
    "#     ylim : tuple, shape (ymin, ymax), optional\n",
    "#         Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "#     cv : int, cross-validation generator or an iterable, optional\n",
    "#         Determines the cross-validation splitting strategy.\n",
    "#         Possible inputs for cv are:\n",
    "\n",
    "#           - None, to use the default 5-fold cross-validation,\n",
    "#           - integer, to specify the number of folds.\n",
    "#           - :term:`CV splitter`,\n",
    "#           - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "#         For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "#         :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "#         or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "#         Refer :ref:`User Guide <cross_validation>` for the various\n",
    "#         cross-validators that can be used here.\n",
    "\n",
    "#     n_jobs : int or None, optional (default=None)\n",
    "#         Number of jobs to run in parallel.\n",
    "#         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "#         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "#         for more details.\n",
    "\n",
    "#     train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "#         Relative or absolute numbers of training examples that will be used to\n",
    "#         generate the learning curve. If the dtype is float, it is regarded as a\n",
    "#         fraction of the maximum size of the training set (that is determined\n",
    "#         by the selected validation method), i.e. it has to be within (0, 1].\n",
    "#         Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "#         Note that for classification the number of samples usually have to\n",
    "#         be big enough to contain at least one sample from each class.\n",
    "#         (default: np.linspace(0.1, 1.0, 5))\n",
    "#     \"\"\"\n",
    "#     if axes is None:\n",
    "#         _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "#     axes[0].set_title(title)\n",
    "#     if ylim is not None:\n",
    "#         axes[0].set_ylim(*ylim)\n",
    "#     axes[0].set_xlabel(\"Training examples\")\n",
    "#     axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "#     train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "#         learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "#                        train_sizes=train_sizes,\n",
    "#                        return_times=True)\n",
    "#     train_scores_mean = np.mean(train_scores, axis=1)\n",
    "#     train_scores_std = np.std(train_scores, axis=1)\n",
    "#     test_scores_mean = np.mean(test_scores, axis=1)\n",
    "#     test_scores_std = np.std(test_scores, axis=1)\n",
    "#     fit_times_mean = np.mean(fit_times, axis=1)\n",
    "#     fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "#     # Plot learning curve\n",
    "#     axes[0].grid()\n",
    "#     axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "#                          train_scores_mean + train_scores_std, alpha=0.1,\n",
    "#                          color=\"r\")\n",
    "#     axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "#                          test_scores_mean + test_scores_std, alpha=0.1,\n",
    "#                          color=\"g\")\n",
    "#     axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "#                  label=\"Training score\")\n",
    "#     axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "#                  label=\"Cross-validation score\")\n",
    "#     axes[0].legend(loc=\"best\")\n",
    "\n",
    "#     # Plot n_samples vs fit_times\n",
    "#     axes[1].grid()\n",
    "#     axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "#     axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "#                          fit_times_mean + fit_times_std, alpha=0.1)\n",
    "#     axes[1].set_xlabel(\"Training examples\")\n",
    "#     axes[1].set_ylabel(\"fit_times\")\n",
    "#     axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "#     # Plot fit_time vs score\n",
    "#     axes[2].grid()\n",
    "#     axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "#     axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "#                          test_scores_mean + test_scores_std, alpha=0.1)\n",
    "#     axes[2].set_xlabel(\"fit_times\")\n",
    "#     axes[2].set_ylabel(\"Score\")\n",
    "#     axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "#     return plt\n",
    "\n",
    "\n",
    "# fig, axes = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "# #X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# title = \"Learning Curves (Naive Bayes)\"\n",
    "# # Cross validation with 100 iterations to get smoother mean test and train\n",
    "# # score curves, each time with 20% data randomly selected as a validation set.\n",
    "# #cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n",
    "\n",
    "# estimator = GaussianNB()\n",
    "# plot_learning_curve(estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),\n",
    "#                     cv=cv, n_jobs=4)\n",
    "\n",
    "# title = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "# # SVC is more expensive so we do a lower number of CV iterations:\n",
    "# cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "# estimator = SVC(gamma=0.001)\n",
    "# plot_learning_curve(estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01),\n",
    "#                     cv=cv, n_jobs=4)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:56.106878Z",
     "start_time": "2020-06-12T10:20:56.105064Z"
    }
   },
   "outputs": [],
   "source": [
    "# # https://towardsdatascience.com/feature-importance-and-forward-feature-selection-752638849962\n",
    "# # Input : Dataframe df with m features, number of required features n\n",
    "# # Output : Set of n features most useful for model performance\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# def forward_feature_selection(x_train, x_cv, y_train, y_cv, n):\n",
    "#     feature_set = []\n",
    "#     for num_features in range(n):\n",
    "#         metric_list = [] # Choose appropriate metric based on business problem\n",
    "#         model = SGDClassifier() # You can choose any model you like, this technique is model agnostic\n",
    "#         for feature in x_train.columns:\n",
    "#             if feature not in feature_set:\n",
    "#                 f_set = feature_set.copy()\n",
    "#                 f_set.append(feature)\n",
    "#                 model.fit(x_train[f_set], y_train)\n",
    "#                 metric_list.append((evaluate_metric(model, x_cv[f_set], y_cv), feature))\n",
    "\n",
    "#         metric_list.sort(key=lambda x : x[0], reverse = True) # In case metric follows \"the more, the merrier\"\n",
    "#         feature_set.append(metric_list[0][1])\n",
    "#     return feature_set\n",
    "\n",
    "# feature_set = forward_feature_selection(X_train, X_val, y_train, y_val, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T00:29:12.404606Z",
     "start_time": "2020-06-10T00:29:11.959353Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:56.466876Z",
     "start_time": "2020-06-12T10:20:56.109908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'admissionweight', 'admissionheight', 'bmi', 'gender_Female',\n",
       "       'ethnicity_African American', 'ethnicity_Asian', 'ethnicity_Caucasian',\n",
       "       'ethnicity_Hispanic', 'ethnicity_Native American',\n",
       "       'ethnicity_Other/Unknown', 'unitstaytype_admit', 'unitstaytype_readmit',\n",
       "       'unitstaytype_transfer', 'verbal', 'motor', 'eyes', 'thrombolytics',\n",
       "       'aids', 'hepaticfailure', 'lymphoma', 'metastaticcancer', 'leukemia',\n",
       "       'immunosuppression', 'cirrhosis', 'activetx', 'ima', 'midur',\n",
       "       'oobventday1', 'oobintubday1', 'diabetes', 'visitnumber', 'heartrate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy = pd.read_csv(dir_read + 'Xy_2020_06_11_1916.csv')\n",
    "Xy = Xy.set_index('patientunitstayid')\n",
    "y = Xy.pop('label')\n",
    "X = Xy.copy()\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:56.568288Z",
     "start_time": "2020-06-12T10:20:56.469387Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Partition and train model\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=1)\n",
    "# #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify=y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:56.574919Z",
     "start_time": "2020-06-12T10:20:56.570413Z"
    }
   },
   "outputs": [],
   "source": [
    "if streamlit_status == 1:\n",
    "    age = 30\n",
    "    admissionweight = 50\n",
    "if streamlit_status == 2:\n",
    "    age = st.slider('Age', 19, 90)\n",
    "    admissionweight = st.slider('Admission Weight (kg)', 40, 300)\n",
    "if (streamlit_status == 1) | (streamlit_status == 2): \n",
    "    input_data = {'age': [age], 'admissionweight': [admissionweight]}\n",
    "    X_test = pd.DataFrame(input_data, columns=['age', 'admissionweight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:56.578943Z",
     "start_time": "2020-06-12T10:20:56.576895Z"
    }
   },
   "outputs": [],
   "source": [
    "# THRESHOLD = 0.01\n",
    "# y_pred = np.where(log_clf.predict_proba(x_train)[:,1] > THRESHOLD, 1, 0) # log_clf: classifier. \n",
    "# # [:,1]: picking out positive probabilities. where that's true, insert value of 1, otherwise 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:56.582746Z",
     "start_time": "2020-06-12T10:20:56.580806Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:56.587106Z",
     "start_time": "2020-06-12T10:20:56.584886Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Tune hyperparameters\n",
    "# hyperparam_grid = {'penalty': ['l1', 'l2'], 'C': np.arange(0.5, 20, 0.5), 'fit_intercept': [True, False]}\n",
    "# logreg =  LogisticRegression(class_weight='balanced')\n",
    "# param = {'C':[0.00001, 0.0001, 0.001]}#0.003,0.005,0.01,0.03,0.05,0.1,0.3,0.5,1,2,3,3,4,5,10,20]}\n",
    "# clf = GridSearchCV(logreg,param,scoring='roc_auc',refit=True,cv=10)\n",
    "# clf.fit(X_train_s,y_train)\n",
    "# print('Best roc_auc: {:.4}, with best C: {}'.format(clf.best_score_, clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:56.592925Z",
     "start_time": "2020-06-12T10:20:56.589047Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_score\n",
    "\n",
    "# %matplotlib notebook\n",
    "# # # Logistic Regression - basic form\n",
    "# # logisticRegr_sc = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "# # logisticRegr_sc.fit(X_train, y_train)\n",
    "# # file_name_pickle = 'model_' + now_to_str() + '.pickle'\n",
    "# # pickle.dump(logisticRegr_sc, open(file_name_pickle, 'wb'))\n",
    "\n",
    "# # # if (streamlit_status == 1) | (streamlit_status == 2):\n",
    "# # #     logisticRegr_sc = pickle.load(open(file_name_pickle_read, 'rb'))\n",
    "\n",
    "# # yhat_logisticRegr_sc = logisticRegr_sc.predict(X_test)\n",
    "# # prob_logisticRegr_sc = logisticRegr_sc.predict_proba(X_test)[:,1]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_sc = scaler.fit_transform(X_train)\n",
    "# X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "# logisticRegr_sc = LogisticRegression(class_weight='balanced')\n",
    "# logisticRegr_sc.fit(X_train_sc, y_train)\n",
    "\n",
    "# yhat_logisticRegr_sc = logisticRegr_sc.predict(X_test_sc)\n",
    "# prob_logisticRegr_sc = logisticRegr_sc.predict_proba(X_test_sc)[:,1]\n",
    "\n",
    "# #lprob_sc = logisticRegr_sc.predict_log_proba(X_test)[:,1]\n",
    "\n",
    "# scores_sc = logisticRegr_sc.score(X_test_sc, y_test)\n",
    "# print(scores_sc)\n",
    "# print('F1 score is: ' + str(f1_score(y_test, yhat_logisticRegr_sc)))\n",
    "\n",
    "# # # Print baseline accuracy\n",
    "# # N0_bl = patient[patient['label']==0].shape[0]\n",
    "# # N1_bl = patient[patient['label']==1].shape[0]\n",
    "# # print('{:d} patients in negative class'.format(N0_bl))\n",
    "# # print('{:d} patients in positive class'.format(N1_bl))\n",
    "# # print('If you predict 0 all the time, accuracy is {:.5f}%'.format(N0_bl/(N0_bl+N1_bl)))\n",
    "\n",
    "# cm = confusion_matrix(list(y_test), yhat_logisticRegr_sc)\n",
    "# plot_confusion_matrix(logisticRegr_sc, X_test, list(y_test))\n",
    "# plot_confusion_matrix(logisticRegr_sc, X_test, list(y_test),  normalize='true')\n",
    "\n",
    "# # ROC curve (https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)\n",
    "# noskill_probs = [0 for _ in range(len(y_test))]\n",
    "# noskill_auc = roc_auc_score(y_test, noskill_probs)\n",
    "# logisticRegr_auc = roc_auc_score(y_test, prob_logisticRegr_sc)\n",
    "# print('No Skill: ROC AUC=%.3f' % (noskill_auc))\n",
    "# print('Logistic: ROC AUC=%.3f' % (logisticRegr_auc))\n",
    "# ns_fpr, ns_tpr, _ = roc_curve(y_test, noskill_probs)\n",
    "# lr_fpr, lr_tpr, _ = roc_curve(y_test, prob_logisticRegr_sc)\n",
    "\n",
    "# plt.figure()\n",
    "# # plot the roc curve for the model\n",
    "# plt.plot(ns_fpr, ns_tpr, linestyle='--')\n",
    "# plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic Regr.')\n",
    "# # axis labels\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# # show the legend\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()\n",
    "\n",
    "# print('prob_sc:  min = ' + str(np.min(prob_logisticRegr_sc)))\n",
    "# print('\\t    max = ' + str(np.max(prob_logisticRegr_sc)))\n",
    "\n",
    "# # Get feature weights and put into dataframe\n",
    "# mydict = {'feature': X_train.columns, 'coef': list(logisticRegr_sc.coef_.reshape(-1,1).flatten())}\n",
    "# features_weights = pd.DataFrame(mydict)\n",
    "# features_weights = features_weights.assign(abs_weight=np.abs(features_weights['coef']))\n",
    "# features_weights.head()\n",
    "# print('accuracy is ' + str(scores_sc))\n",
    "# print('precision is ' + str(precision_score(y_test, yhat_logisticRegr_sc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:56.597308Z",
     "start_time": "2020-06-12T10:20:56.594961Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Feature selection using Recursive Feature Elimination (RFE)\n",
    "# # https://towardsdatascience.com/a-look-into-feature-importance-in-logistic-regression-models-a4aa970f9b0f\n",
    "# from sklearn.feature_selection import RFE\n",
    "# predictors = X_train\n",
    "# selector = RFE(logisticRegr_sc, n_features_to_select = 1) # n_features_to_select gives full ranking of features\n",
    "# selector = selector.fit(predictors, y_train)\n",
    "\n",
    "# order = selector.ranking_\n",
    "# order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T01:51:03.431899Z",
     "start_time": "2020-06-12T01:51:03.420586Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:56.602203Z",
     "start_time": "2020-06-12T10:20:56.599186Z"
    }
   },
   "outputs": [],
   "source": [
    "# # https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "# from sklearn.metrics import plot_roc_curve\n",
    "\n",
    "# cv = StratifiedKFold(n_splits=5)\n",
    "# classifier = LogisticRegression(class_weight = 'balanced')\n",
    "\n",
    "# tprs = []\n",
    "# aucs = []\n",
    "# mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# for i, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "#     X_train, X_test = X.iloc[train_idx],X.iloc[test_idx]\n",
    "#     y_train, y_test = y.iloc[train_idx],y.iloc[test_idx]\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_sc = scaler.fit_transform(X_train)\n",
    "#     X_test_sc = scaler.transform(X_test)\n",
    "    \n",
    "    \n",
    "#     # Tune hyperparameters\n",
    "#     hyperparam_grid = {'penalty': ['l1', 'l2'], 'C': np.arange(0.5, 20, 0.5), 'fit_intercept': [True, False]}\n",
    "#     logreg =  LogisticRegression(class_weight='balanced')\n",
    "#     param = {'C':[0.00001, 0.0001, 0.001]}#0.003,0.005,0.01,0.03,0.05,0.1,0.3,0.5,1,2,3,3,4,5,10,20]}\n",
    "#     clf = GridSearchCV(logreg,param,scoring='roc_auc',refit=True,cv=10)\n",
    "#     clf.fit(X_train_sc,y_train)\n",
    "#     print('Best roc_auc: {:.4}, with best C: {}'.format(clf.best_score_, clf.best_params_))\n",
    "    \n",
    "    \n",
    "#     classifier.fit(X_train_sc, y_train)\n",
    "#     viz = plot_roc_curve(classifier, X_test_sc, y_test,\n",
    "#                          name='ROC fold {}'.format(i),\n",
    "#                          alpha=0.3, lw=1, ax=ax)\n",
    "#     interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "#     interp_tpr[0] = 0.0\n",
    "#     tprs.append(interp_tpr)\n",
    "#     aucs.append(viz.roc_auc)\n",
    "\n",
    "# ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "#         label='Chance', alpha=.8)\n",
    "\n",
    "# mean_tpr = np.mean(tprs, axis=0)\n",
    "# mean_tpr[-1] = 1.0\n",
    "# mean_auc = auc(mean_fpr, mean_tpr)\n",
    "# std_auc = np.std(aucs)\n",
    "# ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "#         label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "#         lw=2, alpha=.8)\n",
    "\n",
    "# std_tpr = np.std(tprs, axis=0)\n",
    "# tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "# tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "# ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "#                 label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "# ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "#        title=\"ROC for Logistic Regression\")\n",
    "# ax.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:20:59.990981Z",
     "start_time": "2020-06-12T10:20:56.603865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Random Forest - vanilla\n",
      "AUC:0.6045407961108826\n",
      "Probability of VTE:  min = 0.0\n",
      "\t    max = 0.18\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classification - basic form\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_vanilla = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=0, n_jobs=4)\n",
    "rfc_vanilla.fit(X_train, y_train)\n",
    "rfc_vanilla_probs = rfc_vanilla.predict_proba(X_test)[:,1]\n",
    "print('******************************')\n",
    "print('Random Forest - vanilla')\n",
    "print('AUC:' + str(roc_auc_score(y_test, rfc_vanilla_probs)))\n",
    "print('Probability of VTE:  min = ' + str(np.min(rfc_vanilla_probs)))\n",
    "print('\\t    max = ' + str(np.max(rfc_vanilla_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:21:09.646535Z",
     "start_time": "2020-06-12T10:20:59.992954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Random Forest - Balanced class weights\n",
      "AUC:0.5730308675840838\n",
      "Probability of VTE:  min = 0.0\n",
      "\t    max = 0.18\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classification - basic form\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_vanilla = RandomForestClassifier(n_estimators=100, random_state=0, criterion='entropy', class_weight='balanced')\n",
    "rfc_vanilla.fit(X_train, y_train)\n",
    "rfc_vanilla_probs = rfc_vanilla.predict_proba(X_test)[:,1]\n",
    "print('******************************')\n",
    "print('Random Forest - Balanced class weights')\n",
    "print('AUC:' + str(roc_auc_score(y_test, rfc_vanilla_probs)))\n",
    "print('Probability of VTE:  min = ' + str(np.min(rfc_vanilla_probs)))\n",
    "print('\\t    max = ' + str(np.max(rfc_vanilla_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:21:11.624485Z",
     "start_time": "2020-06-12T10:21:09.649240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18507284\n",
      "0.39938784\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "model.fit(X_train, y_train)\n",
    "probs_xgb = model.predict_proba(X_test)[:,1]\n",
    "print(min(probs_xgb))\n",
    "print(max(probs_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:21:11.904118Z",
     "start_time": "2020-06-12T10:21:11.631702Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-767bb8196f48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprobs_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs_xgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_sc' is not defined"
     ]
    }
   ],
   "source": [
    "# XGBoost on scaled data\n",
    "import xgboost as xgb\n",
    "model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "model.fit(X_train_sc, y_train)\n",
    "probs_xgb = model.predict_proba(X_test_sc)[:,1]\n",
    "print(min(probs_xgb))\n",
    "print(max(probs_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:21:11.912731Z",
     "start_time": "2020-06-12T10:20:55.210Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# logisticRegr_sc = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "# logisticRegr_sc.fit(X_train, y_train)\n",
    "# file_name_pickle = 'model_' + now_to_str() + '.pickle'\n",
    "# pickle.dump(logisticRegr_sc, open(file_name_pickle, 'wb'))\n",
    "\n",
    "# # if (streamlit_status == 1) | (streamlit_status == 2):\n",
    "# #     logisticRegr_sc = pickle.load(open(file_name_pickle_read, 'rb'))\n",
    "\n",
    "# yhat_logisticRegr_sc = logisticRegr_sc.predict(X_test)\n",
    "# prob_logisticRegr_sc = logisticRegr_sc.predict_proba(X_test)[:,1]\n",
    "# #lprob_sc = logisticRegr_sc.predict_log_proba(X_test)[:,1]\n",
    "\n",
    "# scores_sc = logisticRegr_sc.score(X_test, y_test)\n",
    "# print(scores_sc)\n",
    "# print('F1 score is: ' + str(f1_score(y_test, yhat_logisticRegr_sc)))\n",
    "\n",
    "# # # Print baseline accuracy\n",
    "# # N0_bl = patient[patient['label']==0].shape[0]\n",
    "# # N1_bl = patient[patient['label']==1].shape[0]\n",
    "# # print('{:d} patients in negative class'.format(N0_bl))\n",
    "# # print('{:d} patients in positive class'.format(N1_bl))\n",
    "# # print('If you predict 0 all the time, accuracy is {:.5f}%'.format(N0_bl/(N0_bl+N1_bl)))\n",
    "\n",
    "# # cm = confusion_matrix(list(y_test), yhat_logisticRegr_sc)\n",
    "# # plot_confusion_matrix(logisticRegr_sc, X_test, list(y_test))\n",
    "# # plot_confusion_matrix(logisticRegr_sc, X_test, list(y_test),  normalize='true')\n",
    "\n",
    "# # ROC curve (https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)\n",
    "# noskill_probs = [0 for _ in range(len(y_test))]\n",
    "# noskill_auc = roc_auc_score(y_test, noskill_probs)\n",
    "# logisticRegr_auc = roc_auc_score(y_test, prob_logisticRegr_sc)\n",
    "# print('No Skill: ROC AUC=%.3f' % (noskill_auc))\n",
    "# print('Logistic: ROC AUC=%.3f' % (logisticRegr_auc))\n",
    "# ns_fpr, ns_tpr, _ = roc_curve(y_test, noskill_probs)\n",
    "# lr_fpr, lr_tpr, _ = roc_curve(y_test, prob_logisticRegr_sc)\n",
    "\n",
    "# plt.figure()\n",
    "# # plot the roc curve for the model\n",
    "# plt.plot(ns_fpr, ns_tpr, linestyle='--')\n",
    "# plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic Regr.')\n",
    "# # axis labels\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# # show the legend\n",
    "# plt.legend()\n",
    "# # show the plot\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T12:37:29.377567Z",
     "start_time": "2020-06-05T12:37:29.043910Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:21:11.913833Z",
     "start_time": "2020-06-12T10:20:55.211Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/cost-sensitive-logistic-regression/#:~:text=Logistic%20regression%20does%20not%20support,the%20skewed%20distribution%20into%20account. \n",
    "# Optimizing for F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "567px",
    "left": "690px",
    "right": "20px",
    "top": "8px",
    "width": "536px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
